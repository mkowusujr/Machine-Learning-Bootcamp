{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Machine Learns\n",
    "## The Algorithm\n",
    "1. We start of by making a **prediction** that is completely random.\n",
    "2. Then **calculate the error**, this is where we measure how good our prediction was\n",
    "3. Lastly, learaning step, where we adjust our parameters so we have a smaller error next turn around our initial prediction and **learn from our mistakes**\n",
    "4. Repeat step 1 with our new modified parameters that we found by changing our old ones based on how far off they were\n",
    "5. We keep repeating steps 1-4 until we our satified\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions and Loss Functions\n",
    "Loss and cost functions are methods of measuring the error in machine learning predictions.\n",
    "\n",
    "![example](loss-vs-cost.png)\n",
    "\n",
    "## Loss Functions\n",
    "Loss functions measure the error per observation, whilst cost functions measure the error over all observations.\n",
    "\n",
    "A **Lost Function** is a function that assiociates a cost with a decision. It measures the error between a single prediction and the corresponding actual value\n",
    ">  Loss functions are used to determine the error (aka “the loss”) between the output of our algorithms and the given target value.\n",
    "> In layman’s terms, the loss function expresses how far off the mark our computed output is. \n",
    "\n",
    "### Well known Loss Functions\n",
    "`l1_loss = abs(actual - prediction`  \n",
    "`l2_loss = (actual - prediction) ** 2`\n",
    "\n",
    "## Cost Functions\n",
    "A **Cost Function** is a function which measures the error between predictions and their actual values across the whole dataset. To do this it aggregates the loss values that are calculated per observation.\n",
    "\n",
    "Loss functions are used in regression when finding a line of best fit by minimizing the overall loss of all the points with the prediction from the line.\n",
    "\n",
    "### Well Known Cost Functions\n",
    "Mean Square Error or mse: $MSE = \\frac{1}{n} \\sum(y - \\hat y)^2$  \n",
    "Mean Absolute Error or mae: $\\frac{1}{n} \\sum|yi - xi|$\n",
    "In MAE xi is the observed value in the $i^{th}$ observation and yi is the predicted value for the $i^{th}$ observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "In Data Science and Machine Learning we want to make sure that our loss and cost function have a low error and a high accrucy. Gradient Dscent is the algorithm we use to minimize the error in our loss and cost functions. **Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function.** The gradient descent is the learning alorithm our computers use to reduce the error in their loss and cost functions.\n",
    "\n",
    "In order to find the gradient descent \n",
    "1. We make a radnom guess for the constant in our loss or cost function\n",
    "2. Then we run until a certain stopping condition, for example running for a fixed amount of iterations\n",
    "3. As the alogorithm is running we the derivate of our loss or cost function. This is called finding the gradient. \n",
    "4. Then we travel in the direction of our gradient, heading towards zero. We need a learning rate to determine how big of steps we make as we head towards the gradient. If our gradient is negative we go in the postive direction, if it is postive we go in the negative dirrection.\n",
    "5. Lastly we model our gradient descent alogorithm and decide if we want to modify our initial loss or cost function to change what the model cares about more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Example\n",
    "Our simple cost function:  \n",
    "## $$f(x) = x^2 + x + 1$$  \n",
    "We Are going to use the gradient descent to minimize the error.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Import our modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function is our cost function written as a python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use linspace to create a sequence of values in the start stop range num times that are spaced out equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.linspace(start=-3, stop=3, num=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot our cost function using our generated x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_1, f(x_1))\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim(0, 8)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('f(x)', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the value of x that minimizes the cost. The slope tells us when we have reached our lowest point. We want were the slope is zero.\n",
    "\n",
    "Below is python function that is the derivative of our cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    return 2*x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we graph our diverative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 5])\n",
    "\n",
    "plt.subplot(1, 2, 1) # same row, two columns, index\n",
    "plt.plot(x_1, f(x_1), linewidth=5)\n",
    "plt.title('Cost Function')\n",
    "plt.grid()\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(0, 8)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel(\"f(x)\", fontsize=16)\n",
    "\n",
    "# Chart 2: Derivative\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Slope of Cost Function')\n",
    "plt.plot(x_1, df(x_1), c='skyblue', linewidth=5)\n",
    "plt.grid()\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-4, 5)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('df(x)', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Gradient Descent Algorithmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our calculations more precise we need to run our algorithm longer or make it modify our precision value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = 3 # Our random guess\n",
    "previous_x = 0\n",
    "step_multiplier = 0.1 # Our learning rate\n",
    "precision = 0.00001\n",
    "\n",
    "learning_steps = []\n",
    "gradients_list = []\n",
    "\n",
    "for n in range(500): # Our secondary stopping condition\n",
    "    previous_x = new_x\n",
    "    gradient = df(previous_x) # Find the gradient\n",
    "\n",
    "    learning_steps.append(new_x)\n",
    "    gradients_list.append(gradient)\n",
    "\n",
    "    new_x = previous_x - step_multiplier * gradient # Modify our parameter\n",
    "    step_size = abs(new_x - previous_x) # Fine tuning our learning rate\n",
    "\n",
    "    if (step_size < precision): # Our primary stopping condition\n",
    "        break\n",
    "\n",
    "print('Local mininum occurs at: ', new_x)\n",
    "print('Slope or df(x) value at this point is:', df(new_x))\n",
    "print('f(x) value or cost at this point is:', f(new_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 5])\n",
    "\n",
    "plt.subplot(1, 3, 1) # same row, two columns, index\n",
    "plt.plot(x_1, f(x_1), linewidth=3, alpha=0.6)\n",
    "plt.title('Cost Function')\n",
    "plt.grid()\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim(0, 8)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel(\"f(x)\", fontsize=16)\n",
    "\n",
    "learning_steps_array = np.array(learning_steps)\n",
    "plt.scatter(learning_steps, f(learning_steps_array), color='red', s=40)\n",
    "\n",
    "# Chart 2: Derivative\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Slope of Cost Function')\n",
    "plt.plot(x_1, df(x_1), c='skyblue', linewidth=3, alpha=0.6)\n",
    "plt.grid()\n",
    "plt.xlim(-2, 3)\n",
    "plt.ylim(-3, 6)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('df(x)', fontsize=16)\n",
    "\n",
    "learning_steps_array = np.array(learning_steps)\n",
    "plt.scatter(learning_steps, gradients_list, color='red', s=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The further we are from our minimum point our slope is ver steep. A steep slope means high error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 3: Derivative Close Up\n",
    "plt.figure(figsize=[20, 3])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Derivative Close Up')\n",
    "plt.plot(x_1, df(x_1), c='skyblue', linewidth=3, alpha=0.6)\n",
    "plt.grid()\n",
    "plt.xlim(-0.55, -0.2)\n",
    "plt.ylim(-0.3, 0.8)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('df(x)', fontsize=16)\n",
    "\n",
    "learning_steps_array = np.array(learning_steps)\n",
    "plt.scatter(learning_steps, gradients_list, color='red', s=40)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
